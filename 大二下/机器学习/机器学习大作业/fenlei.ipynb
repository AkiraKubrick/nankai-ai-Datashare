{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-15T09:58:42.529088Z",
     "iopub.status.busy": "2024-06-15T09:58:42.528659Z",
     "iopub.status.idle": "2024-06-15T09:58:51.902155Z",
     "shell.execute_reply": "2024-06-15T09:58:51.900714Z",
     "shell.execute_reply.started": "2024-06-15T09:58:42.529055Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import (train_test_split)\n",
    "\n",
    "# 定义一个函数来读取txt文件并转换为DataFrame\n",
    "def read_text_file(file_path, sep='\\t'):\n",
    "    data = pd.read_csv(file_path, sep=sep, header=None, names=['column1', 'column2', '...']) # 根据你的数据文件结构调整列名\n",
    "    return data\n",
    "\n",
    "# 读取原始txt文件\n",
    "# 假设txt文件中的数据是以制表符分隔的，且没有表头\n",
    "data = read_text_file('./cnews.train.txt')\n",
    "\n",
    "# 随机分割数据集为训练集、验证集和测试集（按比例 8:1:1）\n",
    "train, temp = train_test_split(data, test_size=0.2, random_state=42)\n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 将DataFrame保存为txt文件的函数\n",
    "def save_dataframe_to_txt(dataframe, file_path, sep='\\t'):\n",
    "    dataframe.to_csv(file_path, sep=sep, index=False, header=False)\n",
    "\n",
    "# 将分割后的数据保存为新的txt文件\n",
    "save_dataframe_to_txt(train, 'train.txt')\n",
    "save_dataframe_to_txt(valid, 'valid.txt')\n",
    "save_dataframe_to_txt(test, 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:14:30.432355Z",
     "iopub.status.busy": "2024-06-15T10:14:30.431975Z",
     "iopub.status.idle": "2024-06-15T10:14:30.451872Z",
     "shell.execute_reply": "2024-06-15T10:14:30.450748Z",
     "shell.execute_reply.started": "2024-06-15T10:14:30.432326Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:14:33.307046Z",
     "iopub.status.busy": "2024-06-15T10:14:33.306628Z",
     "iopub.status.idle": "2024-06-15T10:14:33.314429Z",
     "shell.execute_reply": "2024-06-15T10:14:33.312944Z",
     "shell.execute_reply.started": "2024-06-15T10:14:33.307016Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置数据读取、模型、结果保存路径\n",
    "base_dir = './'\n",
    "train_dir = os.path.join(base_dir, 'train.txt')\n",
    "test_dir = os.path.join(base_dir, 'test.txt')\n",
    "val_dir = os.path.join(base_dir, 'val.txt')\n",
    "#vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n",
    "save_dir = 'checkpoints/textcnn'\n",
    "save_path = os.path.join(save_dir, 'best_validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:14:58.032484Z",
     "iopub.status.busy": "2024-06-15T10:14:58.031198Z",
     "iopub.status.idle": "2024-06-15T10:14:58.039864Z",
     "shell.execute_reply": "2024-06-15T10:14:58.038461Z",
     "shell.execute_reply.started": "2024-06-15T10:14:58.032442Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    \"\"\"读取文件数据\"\"\"\n",
    "    contents, labels = [], []\n",
    "    with open(filename,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                label, content = line.strip().split('\\t')\n",
    "                if content:\n",
    "                    contents.append((content))\n",
    "                    labels.append(label)\n",
    "            except:\n",
    "                pass\n",
    "    return contents, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:15:00.852716Z",
     "iopub.status.busy": "2024-06-15T10:15:00.851721Z",
     "iopub.status.idle": "2024-06-15T10:15:01.493042Z",
     "shell.execute_reply": "2024-06-15T10:15:01.491730Z",
     "shell.execute_reply.started": "2024-06-15T10:15:00.852680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'科技': 4045,\n",
       "         '游戏': 4023,\n",
       "         '家居': 4015,\n",
       "         '教育': 4014,\n",
       "         '财经': 4005,\n",
       "         '房产': 3992,\n",
       "         '体育': 3991,\n",
       "         '时政': 3979,\n",
       "         '时尚': 3970,\n",
       "         '娱乐': 3966})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contents, train_labels = read_file(train_dir)\n",
    "test_contents, test_labels = read_file(test_dir)\n",
    "val_counts = Counter(train_labels)\n",
    "val_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:15:10.982100Z",
     "iopub.status.busy": "2024-06-15T10:15:10.981664Z",
     "iopub.status.idle": "2024-06-15T10:15:10.989546Z",
     "shell.execute_reply": "2024-06-15T10:15:10.988013Z",
     "shell.execute_reply.started": "2024-06-15T10:15:10.982066Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#去除文本中的表情字符（只保留中英文和数字）\n",
    "def clear_character(sentence):\n",
    "    pattern1= '\\[.*?\\]'     \n",
    "    pattern2 = re.compile('[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]')   \n",
    "    line1=re.sub(pattern1,'',sentence)\n",
    "    line2=re.sub(pattern2,'',line1)   \n",
    "    new_sentence=''.join(line2.split()) #去除空白\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:15:13.006262Z",
     "iopub.status.busy": "2024-06-15T10:15:13.005847Z",
     "iopub.status.idle": "2024-06-15T10:15:16.115443Z",
     "shell.execute_reply": "2024-06-15T10:15:16.114208Z",
     "shell.execute_reply.started": "2024-06-15T10:15:13.006231Z"
    }
   },
   "outputs": [],
   "source": [
    "train_text=list(map(lambda s: clear_character(s), train_contents))\n",
    "test_text=list(map(lambda s: clear_character(s), test_contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:15:18.743886Z",
     "iopub.status.busy": "2024-06-15T10:15:18.743464Z",
     "iopub.status.idle": "2024-06-15T10:20:31.708571Z",
     "shell.execute_reply": "2024-06-15T10:20:31.707250Z",
     "shell.execute_reply.started": "2024-06-15T10:15:18.743855Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.470 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "train_seg_text=list(map(lambda s: jieba.lcut(s), train_text))\n",
    "test_seg_text=list(map(lambda s: jieba.lcut(s), test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:30:43.649581Z",
     "iopub.status.busy": "2024-06-15T10:30:43.649125Z",
     "iopub.status.idle": "2024-06-15T10:30:43.661205Z",
     "shell.execute_reply": "2024-06-15T10:30:43.659742Z",
     "shell.execute_reply.started": "2024-06-15T10:30:43.649550Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "stop_words_path = \"./stop.txt\"\n",
    "def get_stop_words():\n",
    "    file = open(stop_words_path, 'rb').read().decode('gbk').split('\\r\\n')\n",
    "    return set(file)\n",
    "stopwords = get_stop_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:30:47.909048Z",
     "iopub.status.busy": "2024-06-15T10:30:47.908584Z",
     "iopub.status.idle": "2024-06-15T10:30:47.915410Z",
     "shell.execute_reply": "2024-06-15T10:30:47.914003Z",
     "shell.execute_reply.started": "2024-06-15T10:30:47.909014Z"
    }
   },
   "outputs": [],
   "source": [
    "# 去掉文本中的停用词\n",
    "def drop_stopwords(line, stopwords):\n",
    "    line_clean = []\n",
    "    for word in line:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        line_clean.append(word)\n",
    "    return line_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:30:52.515717Z",
     "iopub.status.busy": "2024-06-15T10:30:52.515296Z",
     "iopub.status.idle": "2024-06-15T10:30:57.630011Z",
     "shell.execute_reply": "2024-06-15T10:30:57.628716Z",
     "shell.execute_reply.started": "2024-06-15T10:30:52.515684Z"
    }
   },
   "outputs": [],
   "source": [
    "train_st_text=list(map(lambda s: drop_stopwords(s,stopwords), train_seg_text))\n",
    "test_st_text=list(map(lambda s: drop_stopwords(s,stopwords), test_seg_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:31:53.703879Z",
     "iopub.status.busy": "2024-06-15T10:31:53.703411Z",
     "iopub.status.idle": "2024-06-15T10:31:53.740953Z",
     "shell.execute_reply": "2024-06-15T10:31:53.738893Z",
     "shell.execute_reply.started": "2024-06-15T10:31:53.703845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:31:57.336731Z",
     "iopub.status.busy": "2024-06-15T10:31:57.336195Z",
     "iopub.status.idle": "2024-06-15T10:31:57.393075Z",
     "shell.execute_reply": "2024-06-15T10:31:57.391830Z",
     "shell.execute_reply.started": "2024-06-15T10:31:57.336686Z"
    }
   },
   "outputs": [],
   "source": [
    "label_train_id=le.transform(train_labels)\n",
    "label_test_id=le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:31:59.636470Z",
     "iopub.status.busy": "2024-06-15T10:31:59.635716Z",
     "iopub.status.idle": "2024-06-15T10:32:00.843005Z",
     "shell.execute_reply": "2024-06-15T10:32:00.841555Z",
     "shell.execute_reply.started": "2024-06-15T10:31:59.636175Z"
    }
   },
   "outputs": [],
   "source": [
    "train_c_text=list(map(lambda s: ' '.join(s), train_st_text))\n",
    "test_c_text=list(map(lambda s: ' '.join(s), test_st_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:32:03.525027Z",
     "iopub.status.busy": "2024-06-15T10:32:03.524404Z",
     "iopub.status.idle": "2024-06-15T10:32:33.852871Z",
     "shell.execute_reply": "2024-06-15T10:32:33.850948Z",
     "shell.execute_reply.started": "2024-06-15T10:32:03.524977Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer(binary=False,token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "train_Data = tfidf_model.fit_transform(train_c_text)\n",
    "test_Data = tfidf_model.transform(test_c_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T10:34:46.681506Z",
     "iopub.status.busy": "2024-06-15T10:34:46.680986Z",
     "iopub.status.idle": "2024-06-15T10:37:00.243045Z",
     "shell.execute_reply": "2024-06-15T10:37:00.241769Z",
     "shell.execute_reply.started": "2024-06-15T10:34:46.681465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9960    0.9960    0.9960       496\n",
      "           1     0.9824    0.9767    0.9796       515\n",
      "           2     0.9554    0.9724    0.9638       507\n",
      "           3     0.9617    0.9638    0.9627       469\n",
      "           4     0.9587    0.9347    0.9466       521\n",
      "           5     0.9722    0.9795    0.9758       536\n",
      "           6     0.9475    0.9611    0.9542       488\n",
      "           7     0.9938    0.9777    0.9857       493\n",
      "           8     0.9418    0.9577    0.9497       473\n",
      "           9     0.9577    0.9482    0.9530       502\n",
      "\n",
      "    accuracy                         0.9668      5000\n",
      "   macro avg     0.9667    0.9668    0.9667      5000\n",
      "weighted avg     0.9669    0.9668    0.9668      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "'''LR模型分类训练'''\n",
    "classifier=LogisticRegression()\n",
    "classifier.fit(train_Data, label_train_id)\n",
    "pred = classifier.predict(test_Data)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label_test_id, pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5175166,
     "sourceId": 8641225,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5215925,
     "sourceId": 8697262,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5216003,
     "sourceId": 8697375,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5172954,
     "sourceId": 8638139,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3460717,
     "sourceId": 6049402,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5216027,
     "sourceId": 8697405,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
